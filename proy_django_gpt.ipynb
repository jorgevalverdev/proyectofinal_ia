{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy python-docx PyPDF2 pandas\n",
    "# python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NER and python help with a code that upload a file (doc, csv, pdf, jason) then make nlt analysis (tokenizacion, remove stopword, lematizacion, steaming, etc) finally  return a dict with the name, dates, places. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import docx\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the pre-trained spaCy model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = []\n",
    "    for para in doc.paragraphs:\n",
    "        text.append(para.text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = []\n",
    "        for page in range(len(reader.pages)):\n",
    "            text.append(reader.pages[page].extract_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to extract text from CSV files\n",
    "def extract_text_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    text = \" \".join(df.astype(str).values.flatten())\n",
    "    return text\n",
    "\n",
    "# Function to extract text from JSON files\n",
    "def extract_text_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    text = json.dumps(data)\n",
    "    return text\n",
    "\n",
    "# Function to perform NER and extract relevant entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    names = []\n",
    "    dates = []\n",
    "    places = []\n",
    "\n",
    "    # Iterate over recognized entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            names.append(ent.text)\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            # Convert date to standardized format if possible\n",
    "            try:\n",
    "                date_obj = datetime.strptime(ent.text, '%B %d, %Y')  # Example for month-day-year format\n",
    "                dates.append(date_obj.strftime('%Y-%m-%d'))\n",
    "            except ValueError:\n",
    "                dates.append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":  # GPE: Geopolitical Entity (places)\n",
    "            places.append(ent.text)\n",
    "    \n",
    "    return {\n",
    "        \"names\": names,\n",
    "        \"dates\": dates,\n",
    "        \"places\": places\n",
    "    }\n",
    "\n",
    "# Function to process the file and return entities\n",
    "def process_file(file_path):\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    text = \"\"\n",
    "\n",
    "    if file_extension == 'docx':\n",
    "        text = extract_text_docx(file_path)\n",
    "    elif file_extension == 'pdf':\n",
    "        text = extract_text_pdf(file_path)\n",
    "    elif file_extension == 'csv':\n",
    "        text = extract_text_csv(file_path)\n",
    "    elif file_extension == 'json':\n",
    "        text = extract_text_json(file_path)\n",
    "    else:\n",
    "        return {\"error\": \"Unsupported file format\"}\n",
    "\n",
    "    # Extract and return entities\n",
    "    return extract_entities(text)\n",
    "\n",
    "# Example Usage\n",
    "file_path = 'path_to_your_file.pdf'  # Replace with your file path\n",
    "result = process_file(file_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'stopwords', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tree import Tree\n",
    "import spacy\n",
    "\n",
    "# Initialize spaCy for NER (Named Entity Recognition)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper function for reading files\n",
    "def read_file(file_path):\n",
    "    ext = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    # Read DOCX files\n",
    "    if ext == 'docx':\n",
    "        doc = docx.Document(file_path)\n",
    "        text = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "    # Read CSV files (assuming the text is in the first column)\n",
    "    elif ext == 'csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "        text = ' '.join(df.iloc[:, 0].dropna().astype(str))\n",
    "    # Read PDF files\n",
    "    elif ext == 'pdf':\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "    # Read JSON files (assuming JSON has a key 'text')\n",
    "    elif ext == 'json':\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            text = data.get('text', '')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type!\")\n",
    "    return text\n",
    "\n",
    "# Tokenization, stopword removal, lemmatization, stemming\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize and Stem\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "def extract_entities(text):\n",
    "    # Use SpaCy NER for extracting names, places, dates, etc.\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entities = {'names': [], 'places': [], 'dates': []}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            entities['names'].append(ent.text)\n",
    "        elif ent.label_ == 'GPE':  # Geopolitical Entity (places)\n",
    "            entities['places'].append(ent.text)\n",
    "        elif ent.label_ == 'DATE':\n",
    "            entities['dates'].append(ent.text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Main function to upload file, process text and extract entities\n",
    "def process_file(file_path):\n",
    "    # Read the file content\n",
    "    text = read_file(file_path)\n",
    "    \n",
    "    # Preprocess the text\n",
    "    lemmatized_tokens, stemmed_tokens = preprocess_text(text)\n",
    "    \n",
    "    # Extract named entities\n",
    "    entities = extract_entities(text)\n",
    "    \n",
    "    # Create a result dictionary\n",
    "    result = {\n",
    "        'file_name': file_path.split('/')[-1],\n",
    "        'entities': entities,\n",
    "        'lemmatized_tokens': lemmatized_tokens,\n",
    "        'stemmed_tokens': stemmed_tokens\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "file_path = 'path_to_your_file.pdf'  # Replace with your file path\n",
    "result = process_file(file_path)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
