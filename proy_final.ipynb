{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy python-docx PyPDF2 pandas\n",
    "# python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NER and python help with a code that upload a file (doc, csv, pdf, jason) then make nlt analysis (tokenizacion, remove stopword, lematizacion, steaming, etc) finally  return a dict with the name, dates, places. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxent_ne_chunker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'stopwords', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m    102\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobal American Report.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your file path\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[3], line 89\u001b[0m, in \u001b[0;36mprocess_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     86\u001b[0m lemmatized_tokens, stemmed_tokens \u001b[38;5;241m=\u001b[39m preprocess_text(text)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Extract named entities\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m entities \u001b[38;5;241m=\u001b[39m \u001b[43mextract_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Create a result dictionary\u001b[39;00m\n\u001b[0;32m     92\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m: file_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m'\u001b[39m: entities,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: lemmatized_tokens,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmed_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: stemmed_tokens\n\u001b[0;32m     97\u001b[0m }\n",
      "Cell \u001b[1;32mIn[3], line 66\u001b[0m, in \u001b[0;36mextract_entities\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_entities\u001b[39m(text):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Use SpaCy NER for extracting names, places, dates, etc.\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m(text)\n\u001b[0;32m     68\u001b[0m     entities \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplaces\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdates\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tree import Tree\n",
    "###import spacy\n",
    "\n",
    "# Initialize spaCy for NER (Named Entity Recognition)\n",
    "##nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper function for reading files\n",
    "def read_file(file_path):\n",
    "    ext = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    # Read DOCX files\n",
    "    if ext == 'docx':\n",
    "        doc = docx.Document(file_path)\n",
    "        text = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "    # Read CSV files (assuming the text is in the first column)\n",
    "    elif ext == 'csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "        text = ' '.join(df.iloc[:, 0].dropna().astype(str))\n",
    "    # Read PDF files\n",
    "    elif ext == 'pdf':\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "    # Read JSON files (assuming JSON has a key 'text')\n",
    "    elif ext == 'json':\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            text = data.get('text', '')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type!\")\n",
    "    return text\n",
    "\n",
    "# Tokenization, stopword removal, lemmatization, stemming\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize and Stem\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_tokens, stemmed_tokens\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "def extract_entities(text):\n",
    "    # Use SpaCy NER for extracting names, places, dates, etc.\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entities = {'names': [], 'places': [], 'dates': []}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            entities['names'].append(ent.text)\n",
    "        elif ent.label_ == 'GPE':  # Geopolitical Entity (places)\n",
    "            entities['places'].append(ent.text)\n",
    "        elif ent.label_ == 'DATE':\n",
    "            entities['dates'].append(ent.text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Main function to upload file, process text and extract entities\n",
    "def process_file(file_path):\n",
    "    # Read the file content\n",
    "    text = read_file(file_path)\n",
    "    \n",
    "    # Preprocess the text\n",
    "    lemmatized_tokens, stemmed_tokens = preprocess_text(text)\n",
    "    \n",
    "    # Extract named entities\n",
    "    entities = extract_entities(text)\n",
    "    \n",
    "    # Create a result dictionary\n",
    "    result = {\n",
    "        'file_name': file_path.split('/')[-1],\n",
    "        'entities': entities,\n",
    "        'lemmatized_tokens': lemmatized_tokens,\n",
    "        'stemmed_tokens': stemmed_tokens\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "file_path = 'Global American Report.pdf'  # Replace with your file path\n",
    "result = process_file(file_path)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
